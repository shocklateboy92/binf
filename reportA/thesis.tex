\documentclass[a4paper,12pt]{report}

\usepackage{setspace}
\onehalfspacing

\usepackage{anysize}
\marginsize{2cm}{2cm}{2cm}{2cm}

\usepackage{graphicx}
\usepackage{caption}
\usepackage[utf8]{inputenc}

\usepackage{csquotes}
\usepackage[australian]{babel}

\usepackage[backend=biber]{biblatex}
\addbibresource{thesis.bib}

\usepackage{hyperref}

\usepackage{pgfplots}
\pgfplotsset{width=\textwidth,compat=1.9}

\usepgfplotslibrary{external}
\tikzexternalize

\author{Lasath Fernando}
\title{Thesis A}
\date{15/10/14}

\begin{document}
\maketitle


\chapter{Introduction}
\section{Combination Process}
\section{Hidden Markov Models}
\section{iHMMuneAlign}
Developed in 2007 by a team of researchers at UNSW, iHMMuneAlign attempts to simulate this process \autocite{iHMMuneAlign}.
It represents the combination process of immunoglobulin heavy chain genes as a hidden markov model.


\chapter{Analysis}
Since the high level aim of this project is to improve the performance of iHMMuneAlign, it was necessary to analyze the existing implementation in order to gain insight into its structure, as well as to create realistic aims. 

\section{Run Time}
One of the primary attributes of performance, is run time. We begin by measuring the run time of the existing implementation and -more importantly- how it grows with workload.

The current implementation consists of a Java executable and a shell script, to allow running in batches. The Java executable (which does the actual processing) takes a range of input from the script and attempts to process that many in paralell. It then invokes Java program after breaking up the entire range into batches of a given size.

Since we are interested in the rate the total runtime changes with workload as well with increasing paralellism, both were measured. It was run with a total workload ranging from 1 sequences to 100 sequences, with batch size ranging from 1 to 8. The results are shown in \autoref{fig:runtime}.

%TODO: describe machine specs

\subsection{Interpretation of Results}
With a batch size of 1, the expected linear increase was present. However, increasing the batch size shows a more sporadic decrease in runtime. This is to be expected, since concurrent programs tend to be very non-deterministic in execution time. However, there is an overall decrease in total execution time, as the batch size increases.

%% TODO: replace jit reference with paper from IBM
While this is in part due to the parallel nature of the existing implementation, it is also due to the overhead from invoking the Java program reducing. In each invocation, the Java Virtual Machine (JVM) has to be created, the program loaded (and JIT compiled \autocite{jit}), and the program itself needs to read the repitoire files etc. Since all this only has to be done once per batch, reducing the number of batches also reduced the total work needed to be done significantly. This can be seen in \autoref{fig:usertime}, which shows that the CPU time (total time that all the CPUs spent working, rather than waiting for each other, and waiting for disk/system operations) also reduces with batch size.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      axis lines = left,
      xlabel = Total sequences,
      ylabel = Time (Seconds),
    ]

      \input{data}
    \end{axis}
  \end{tikzpicture}
  \caption{Total execution time (in seconds) to process the number of sequences. $n$ is the number of sequences per invocation of the program}
  \label{fig:runtime}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      axis lines = left,
      xlabel = Total sequences,
      ylabel = Time (Seconds),
    ]

      \input{data2}
    \end{axis}
  \end{tikzpicture}
  \caption{Total CPU time (in seconds) to process the number of sequences. $n$ is the number of sequences per invocation of the program}
  \label{fig:usertime}
\end{figure}


\section{Profiling}
In order to determine the most appropriate approaches for improving performance and overcoming the limitations of the original implementation, further analysis is required. Specifically, we are interested in which parts of the program consume the most CPU time and memory, as well as how they increase with the workload.

The method for collecting this data is called profiling. Since the orginal implementation of iHMMuneAlign was written in Java, we can use various interfaces in the Java Virtual Machine (JVM) to gather this data. The JVM has an interface called Java Native Interface (JNI)\autocite{jni}, that allows external programs and native libraries (usually written in C/C++) to interact with Java objects. Alongside that, from Java verison 1.5 onwards, the JVM also provides another interface, called Java Virtual Machine Tool Interface (JVMTI)\autocite{jvmti}, which allows external tools to be notified as certain events (such as object allocation, method call) occur during execution.

In \autoref{fig:callgraph}, the data for a run of iHMMuneAlign with 6 sequences, instrumented by jProfiler (a profiler implemented with the above method) is shown.

\begin{figure}
 \caption{The call graph generated by jProfiler, for a single run of iHMMuneAlign}
 \label{fig:callgraph}
\end{figure}


\chapter{Proposed Solution}

\section{Re-implementing From Scratch}
Based on the above analysis, it is clear that a major structural overhaul will be required in order to meaningfully proceed any further. The primary reasons for it is that the existing code is not structured in an easily maintainable way, and the primary authors are not longer available to assist us. 

Re-implementing from scratch will allowed us to choose a different langauge than the current one and as a result it will be implementated in C++. C++ is still the industry standard for constructing large, high performance software\autocite{cpp}, as it contains concepts from high level lanugages, while still allowing for low level optimizations, including memroy management. 

The initial version of the new implementation will be loosely based off the already existing implementation. Once the code is rewritten and structured in a manner that lends itself to easy modification, the real optimization (such as threading, and caching computation) can begin.

\section{Dependencies of iHMMuneAlign}
Apart from the codebase, the current implementation has several dependencies that perform significant amounts of computation. This section will outline how the new implemntation will replicate their features.

\subsection{BioJava}
BioJava \autocite{biojava}is a general purpose Bioinformatics library, that contains several useful features for iHMMuneAlign. However, since iHMMuneAlign was written, BioJava has undergone a complete restructuring, and most -if not all- the modules the original implementation uses have been deprecated.

\subsubsection{Fasta Reader}
FASTA was originally the format used by a program that bore its name to store databases of DNA sequences \autocite{fasta}. These days, it is commonly used in Bioinformatics to represent large quantities of gene sequences within a file. BioJava could parse files FASTA format and convert them to its in-memory representation, which were then used by iHMMuneAlign. Since we no longer intend to use BioJava, an alternate method was required to parse the inputs and repitoire files for each genes was required. Since the FASTA format is a very simple format, a custom parser could be built quite easily. In addition, the NCBI C++ Toolkit \autocite{ncbi-fasta} can also be used to trivially parse FASTA files.

\subsubsection{Bioinformatic Data Structures}
BioJava contains a large collection of pre-built data structures to represent commonly occuring biological objects and concepts (E.g. RichSequenece, FiniteAlphabet). Here we have several viable solutions.

\paragraph{Bio++}
is a set of general purpose Bioinformatics libraries written in C++ \autocite{bpp}. Much like BioJava, it also provieds various pre built data structures that represent various biological entities, like gene sequences. However, how well they correspond to the used data structures from BioJava and how useful they are when re-implementing is yet to be seen.

%% sensible (text),from this point on
\paragraph{Qt}
is a C++ framework, originally created by Trolltech and now maintained by Digia. It has various components that assist in the rapid building of user interfaces, as well complex systems and algorithms. While they are less specific to biology, they are designed for speed and efficiency. Once the initial version is built using Bio++, if Bio++'s data structures are deemed a significant performance bottleneck, new ones will be custom built using Qt's support structure.

\paragraph{STL}
The Standard Template Library (STL) that accompanies C++ also contains a series of very high performance, generic data structures and algorithms \autocite{stl}. However, it is widely considered to be less user friendly than its alternatives, like Qt. For this reason, Qt's data structures were selected as the primary candidate for a custom implementation, and if they cause a significant performance limitation, they will be replaced by their STL counterparts.

\paragraph{Boost}
is another, highly comprehensive set of libraries that extend the functionality of the STL. Select features from Boost often end up in subsequent revisions of the STL. Should functionality of the STL be insufficent to replace the Qt implementation once it's been deemed a performance bottleneck, Boost will be used to compensate.

\subsection{DP Module}
$<$Note: determine if this deserves its own section$>$

\section{Concurrency}
On modern computers, getting the most performance requires a program to be concurrent \autocite{freelunch}. Even the original implementation did use multiple threads of execution, using Java's built-in threading library.

\printbibliography

\end{document}
