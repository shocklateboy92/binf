\chapter{Evaluation}\label{ch:eval}

\section{Goals}
To reiterate, the primary goal of this project was to improve the performance of iHMMuneAlign; the HMM based gene alignment tool. The best method of achieving that was determined to be re-implementing from scratch, for reasons explained in \secref{sec:reimplementing}.

The secondary goal was to improve the maintainability of iHMMuneAlign; to avoid necessitating such rewrites in future. These two were quite well achieved, as will be demonstrated in the following sections.

However, due to time constraints, not all features of the old implementation could be recreated in the new one. This was a conscious decision -- we prioritised performance and maintainability over feature completeness, in order to demonstrate the possibilities.


\section{Current State}
As detailed in \secref{sec:blast-impl}, iHMMuneAlign no longer does the BLAST pre-alignment for the IGHV gene directly. For the moment, there is a BASH script which invokes \code{blastn} and then iHMMuneAlign with its output. For all the benchmarks in this section, invoking the new implementation of iHMMuneAlign will actually refer to launching this script.

Currently, iHMMuneAlign does not support gene sequences with gaps in them. If it encounters one during alignment, it will abort that sequence. This is an important feature, and will be re-added at a later date. 

It is important to take note however, that this will not effect the performance comparisons below: the inputs will not result in any gaps during alignment, and the performance penalty of checking if gaps exist is paid by both implementations.

Presently, iHMMuneAlign does not model N and P addition. The previous implementation models N-addition; with evidence it also modelled P-addition at an earlier point in time.
Modelling these require having extra states in the model, which will make a significant impact on total workload (and thus, performance). In order to allow for a fair comparison of performance, extra D and J states are added to the model. This is of course, a detriment to correctness and part of the concious effort to prioritise performance at the expense of correctness.

Due to the limitations listed above, the output of iHMMuneAlign no longer matches exactly with the previous implementation. However, the overall improvement of iHMMuneAlign is an ongoing process and all important features will be added back in.

%TODO: mention lack of kappa/lambda chain

\vfil
\pagebreak
\section{Performance}

\begin{wrapfigure}[26]{L}{0.4\textwidth}
	\begin{tikzpicture}
	\begin{axis}[
	boxplot/draw direction=y,
	xtick={1,2,3},
	xticklabels={Old Impl., {1-Thread}, 12-Threads},
	x tick label style={rotate=90},
	width=0.4\textwidth,
	height=0.9\textheight,
	cycle list name=color list,
	ylabel=Time Taken (in Seconds)
	]
	
	\input{eval-runtime-both}	
	
	\end{axis}
	\end{tikzpicture}
	\caption{Total (Wall-clock) time taken to process 300 sequences}
	\label{fig:eval-boxplots}
\end{wrapfigure}

The new implementation of iHMMuneAlign was designed from scratch with performance in mind, dictating choice of language, frameworks/libraries, as well as various architectural decisions. 

Note that all the benchmarks and analysis detailed in this section were performed on a machine with an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU E5-1650 v3 @ 3.50GHz, 32GiB RAM and a Samsung\textsuperscript{\textregistered} 840 Pro SSD. The CPU contains 6 physical cores, and uses Intel\textsuperscript{\textregistered} Hyper-Threading technology to provide 12 logical cores.

\subsection{Runtime}
While there are many different aspects when it comes to the performance of computer programs, in this case we care about run-time specifically. 

\subsubsection{Method Details}

The benchmark to measure and compare the total run-times was fairly straightforward.

\paragraph{Input Data} \hfill \\
was chosen meticulously, to ensure processing each sequence would only require features supported by both implementations. To reach a sufficient workload, duplicating sequences were required; while not ideal, it should suffice for our experiments.

\paragraph{Execution Environment} \hfill \\
was the machine (hardware detailed above) running Arch Linux (Kernel 4.0.4). Furthermore, the benchmarks were run within runlevel 3 \autocite{runlevels}; a lower level environment used during machine initialisation. As such, there were no other non-essential tasks being performed on the system and should minimise interference.

\paragraph{Measurement} \hfill \\
of results was performed using the shell's inbuilt \code{time} utility. This invokes a given program and uses the various operating system features to determine the following:
\begin{description}
	\item[Real time] The total (Wall-clock) time from program invocation to termination.
	\item[CPU time] Sum of the amount of time spent actually working on behalf of the program by each CPU
	\item[System time] The amount of time spent by the kernel working on behalf of said program
\end{description}

\paragraph{Sample Size} \hfill \\
A sufficiently hight sample size is required to show that the recorded data are statistically significant i.e. not by random chance. However, since a large number of tests are performed in these benchmarks, there is a trade-off between statistical precision and completing in a reasonable time.
Ultimately, a value of 20 were most suitable -- each program was invoked 20 times per configuration.

\subsubsection{Interpretation of Results}
The total execution times for both implementations can be seen on \autoref{fig:eval-boxplots}. Since the previous implementation will only use 2 threads, it was run with 2 threads. 
The new implementation however, will create and use as many threads as there are available CPU cores on the target machine (12 in this case).

Also note that the graph is a box plot for all 3 measurements, rather than just the first two. However, in the case of the third measurement (new implementation without any artificial constraints) this is difficult to see, as the variance is so low relative to the difference in execution time between it and old implementation.

It is readily apparent from the graph that the new implementation is approximately 5 times faster on this machine. While that factor will increase on more powerful machines, it should also be noted that the new implementation is more efficient than the previous.

This can be seen by comparing the CPU times between executions, as presented in \autoref{fig:eval-user-time}. The mean CPU time for the new implementation (restricted to two threads) was 62.24 seconds, while the old one used 230.55 seconds. In simple terms, this means that the old implementation must to roughly 4 times the work to end up with the same results. This can be attributed to various factors including the overhead of Java, as well as the (non performance oriented) structure of the old program. 

\subsection{Parallelism}
\label{sec:eval-parallelism}
As described in detail in \secref{sec:concurrency}, parallelism is essential for modern high performance applications. While total computational power in modern machines are increasing faster than ever, single threaded performance has stagnated. Clock speeds are no longer increasing significantly; all that extra power comes in new cores, which require increasing parallelism to leverage. 

It is clear from the analysis performed in \secref{sec:threadprof}, that the previous implementation has a very low degree of parallelism. In fact, it would cease to gain any benefit from a machine that has more than two cores, which nearly all modern machines do. 

\autoref{fig:i2-time} shows that there \lipsum[1]

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
		axis lines = left,
		xlabel = Threads,
		ylabel = Time (Seconds),
		xmin = 0,
		ymin = 10
	]
	
	\input{eval-threads}
	\end{axis}
	\end{tikzpicture}
	\caption{Total (Wall-clock) time taken to process 300 sequences, using a variable number of threads}
	\label{fig:i2-time}
\end{figure}

\leavevmode
\section{Maintenance}
The second goal of this project is to make the code for iHMMuneAlign much more maintainable, allowing others to add new features as well as perform further optimisations.


\subsection{Revision Control}
The previous codebase had been maintained since its inception in 2007 until present day by various people. Many features have clearly been added and removed since then, resulting in large chunks of dead (unused) or commented out code.

If this project were under revision control, it could have for code to have been deleted instead of being commented out (they can still be viewed later using revision history). Furthermore, it would have also allowed new features and experiments to have been constructed in branches; only entering the main branch when they were deemed suitable.

The new implementation of iHMMuneAlign has used the Git revision control system since its creation, all the progress that lead to the feature-set and code structure it has today is recorded in its history. As a result, it could avoid several issues, going forward, that plague the previous codebase.

\leavevmode
\subsection{Separation of Logic}
One of the largest barriers to understanding the source code of the previous implementation is its lack of clear structure. Several key parts of the logic behind model generation are distributed throughout the code, making components dependent on each other in subtle ways. 

This is not only makes the code much harder to follow, but also virtually impossible to modify without introducing subtle bugs; increasing the workload as the developer must watch for this.

The structure of the new implementation is laid out in detail in \secref{sec:pipeline}; with a dedicated section in the code for each stage in the pipeline, plus an initialisation section. This lack of inter-dependence (loose coupling) not only resulted in ease of optimisation (especially with regards to parallelism), but ease of understanding -- a developer only needs to keep the details of that stage in mind.
	
\subsection{Unit Tests}
The previous implementation did not contain any form of automated testing -- there were some debugging code, but it still required some sort of manual inspection.

Another consequence of the code being highly interdependent is that it is exceptionally difficult to test an individual component. If unit tests had been written from that start, it would have forced the original developers to structure the code such that components can be tested individually.

The new implementation has a series of unit tests, that were written along with the features they test. This will ensure that as developers add new features, or perform new optimisations, any changes in behaviour introduced will cause tests to fail. Not only will this encourage developers to make new improvements by reducing the risk of breaking features, but it will mean they waste less time tracking down bugs.


While there are several widely accepted techniques for writing maintainable code --which I've followed--, there is no real way of measuring code quality quantitatively; in reality only time will tell.
