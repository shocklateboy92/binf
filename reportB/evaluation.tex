\chapter{Evaluation}\label{ch:eval}

\section{Goals}
Explain the dynamic goals that were defined, and how they were designed to be scaled back as complications arose, and time permitted. 

Also should I explain that those goals were originally set with a group project in mind, and that's the primary reason they had to be scaled back so far?

\section{Current State}
Will describe the features that the new version supports already, will support in the future.

Should I emphasise that this is a work in progress?

% doesn't support N-states, but has extra D states to compensate.
% aborts on gaps - has same penalty for detection

\begin{wrapfigure}[25]{l}{0.4\textwidth}
	\begin{tikzpicture}
	\begin{axis}[
	boxplot/draw direction=y,
	xtick={1,2,3},
	xticklabels={Old Impl., {1-Thread}, 12-Threads},
	x tick label style={rotate=90},
	width=0.4\textwidth,
	height=0.9\textheight,
	cycle list name=color list,
	ylabel=Time Taken (in Seconds)
	]
	
	\input{eval-runtime-both}	
	
	\end{axis}
	\end{tikzpicture}
	\caption{Total (Wall-clock) time taken to process 300 sequences}
	\label{fig:eval-boxplots}
\end{wrapfigure}

\section{Performance}

The new implementation of iHMMuneAlign was designed from scratch with performance in mind, dictating choice of language, frameworks/libraries, as well as various architectural decisions. 

Note that all the benchmarks and analysis detailed in this section were performed on a machine with an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU E5-1650 v3 @ 3.50GHz, 32GiB RAM and a Samsung\textsuperscript{\textregistered} 840 Pro SSD. The CPU contains 6 physical cores, and uses Intel\textsuperscript{\textregistered} Hyper-Threading technology to provide 12 logical cores.

\subsection{Runtime}
While there are many different aspects when it comes to the performance of computer programs, in this case we care about run-time specifically. 

Fortunately, this can be measured quantitatively and compared easily using benchmarks. For this, 300 gene sequences were chosen that only required features supported by both implementations. I 

\subsection{Parallelism}
As described in detail in \secref{sec:concurrency}, parallelism is essential for modern high performance applications. While total computational power in modern machines are increasing faster than ever, single threaded performance has stagnated. Clock speeds are no longer increasing significantly; all that extra power comes in new cores, which require increasing parallelism to leverage. 

It is clear from the analysis performed in \secref{sec:threadprof}, that the previous implementation has a very low degree of parallelism. In fact, it would cease to gain any benefit from a machine that has more than two cores, which nearly all modern machines do. 

\autoref{fig:i2-time} shows that there 

\begin{figure}
	\centering
	\begin{tikzpicture}
	\begin{axis}[
		axis lines = left,
		xlabel = Threads,
		ylabel = Time (Seconds),
		xmin = 0,
		ymin = 0
	]
	
	\input{eval-threads}
	\end{axis}
	\end{tikzpicture}
	\caption{Total (Wall-clock) time taken to process 300 sequences, using a variable number of threads}
	\label{fig:i2-time}
\end{figure}

\leavevmode
\section{Maintenance}
The second goal of this project is to make the code for iHMMuneAlign much more maintainable, allowing others to add new features as well as perform further optimisations.


\subsection{Revision Control}
The previous codebase had been maintained since its inception in 2007 until present day by various people. Many features have clearly been added and removed since then, resulting in large chunks of dead (unused) or commented out code.

If this project were under revision control, it could have for code to have been deleted instead of being commented out (they can still be viewed later using revision history). Furthermore, it would have also allowed new features and experiments to have been constructed in branches; only entering the main branch when they were deemed suitable.

The new implementation of iHMMuneAlign has used the Git revision control system since its creation, all the progress that lead to the feature-set and code structure it has today is recorded in its history. As a result, it could avoid several issues, going forward, that plague the previous codebase.

\leavevmode
\subsection{Separation of Logic}
One of the largest barriers to understanding the source code of the previous implementation is its lack of clear structure. Several key parts of the logic behind model generation are distributed throughout the code, making components dependent on each other in subtle ways. 

This is not only makes the code much harder to follow, but also virtually impossible to modify without introducing subtle bugs; increasing the workload as the developer must watch for this.

The structure of the new implementation is laid out in detail in \secref{sec:pipeline}; with a dedicated section in the code for each stage in the pipeline, plus an initialisation section. This lack of inter-dependence (loose coupling) not only resulted in ease of optimisation (especially with regards to parallelism), but ease of understanding -- a developer only needs to keep the details of that stage in mind.
	
\subsection{Unit Tests}
The previous implementation did not contain any form of automated testing -- there were some debugging code, but it still required some sort of manual inspection.

Another consequence of the code being highly interdependent is that it is exceptionally difficult to test an individual component. If unit tests had been written from that start, it would have forced the original developers to structure the code such that components can be tested individually.

The new implementation has a series of unit tests, that were written along with the features they test. This will ensure that as developers add new features, or perform new optimisations, any changes in behaviour introduced will cause tests to fail. Not only will this encourage developers to make new improvements by reducing the risk of breaking features, but it will mean they waste less time tracking down bugs.


While there are several widely accepted techniques for writing maintainable code --which I've followed--, there is no real way of measuring code quality quantitatively; in reality only time will tell.
